# Data Platform POC

A proof-of-concept modern data platform demonstrating a simple yet scalable lakehouse architecture using pg_lake (PostgreSQL with Iceberg support), pgduck_server (PostgreSQL-protocol server for DuckDB), MinIO (object storage), and SQLMesh (data transformation framework).

> DISCLAIMER: This README is generated by AI based on the repository, some information may not be accurate, if you find inaccurate information please verify with the official documentation.

## Architecture Overview

This project implements a streamlined data lakehouse architecture that maintains cloud-native principles while remaining simple to run locally:

### Core Components

- **pg_lake**: PostgreSQL-based lakehouse that serves as the Apache Iceberg catalog
- **pgduck_server**: Specialized server using PostgreSQL wire protocol to execute queries on DuckDB for high-performance analytics
- **MinIO**: S3-compatible object storage for storing actual data files
- **SQLMesh**: Data transformation framework for managing SQL-based data models with CI/CD capabilities
- **Apache Iceberg**: Table format for large analytical datasets (via pg_lake)

### Architecture Flow

```
┌─────────────┐
│   SQLMesh   │ ─── Transforms data ───┐
└─────────────┘                        │
                                       ▼
┌──────────────────────────────────────────┐         ┌─────────────┐
│         pg_lake (PostgreSQL)             │ ◄─────► │    MinIO    │
│  • Iceberg SQL Catalog                   │         │ (S3 Storage)│
│  • Metadata Management                   │         └─────────────┘
└──────────────────────────────────────────┘               │
                                                            │
┌──────────────────────────────────────────┐               │
│      pgduck_server (Port 5332)           │               │
│  • PostgreSQL Wire Protocol              │               │
│  • DuckDB Query Engine                   │ ◄─────────────┘
│  • High-Performance Analytics            │
└──────────────────────────────────────────┘
```

### Why This Architecture?

**Simplicity**: Two focused services - PostgreSQL for catalog, DuckDB for queries - instead of complex distributed systems.

**Performance**: pgduck_server leverages DuckDB's columnar execution engine for blazing-fast analytical queries while maintaining PostgreSQL client compatibility.

**Standards-Based**: Uses Apache Iceberg table format, ensuring compatibility with the broader data ecosystem.

**Cloud-Ready**: The same concepts scale directly to cloud infrastructure (see "Scaling to Cloud" section).

## Technology Stack

- **Python 3.12+**: Primary programming language
- **SQLMesh**: Data transformation and workflow orchestration
- **PostgreSQL (pg_lake)**: Iceberg catalog and metadata management
- **pgduck_server**: PostgreSQL-protocol server for DuckDB query execution
- **DuckDB**: High-performance analytical query engine
- **Apache Iceberg**: Open table format for analytical datasets
- **MinIO**: S3-compatible object storage
- **Docker Compose**: Container orchestration

## Project Structure

```
dolanan-data/
├── sqlmesh/                    # SQLMesh project directory
│   ├── models/                 # SQL model definitions
│   │   └── *.sql              # Data transformation models
│   ├── audits/                # Data quality audits
│   ├── macros/                # Reusable SQL macros
│   ├── seeds/                 # Static seed data
│   ├── tests/                 # Model tests
│   └── config.yaml            # SQLMesh configuration
├── pg_lake/                   # PostgreSQL/pg_lake setup
│   └── init/                  # Initialization scripts
├── docker-compose.yml         # Infrastructure definition
├── pyproject.toml            # Python dependencies
└── README.md                 # This file
```

## Features

### Dual-Service Lakehouse
- **Iceberg Catalog**: pg_lake manages all table metadata using PostgreSQL
- **Query Engine**: pgduck_server provides high-performance analytical queries via DuckDB
- **PostgreSQL Compatibility**: pgduck_server uses PostgreSQL wire protocol, so any PostgreSQL client works
- **Separation of Concerns**: Catalog and query execution are cleanly separated

### SQLMesh Integration
- **Gateway**: pgduck_server connection for analytical workloads
- **State Backend**: PostgreSQL (pg_lake) for state management
- **Model Defaults**: Configurable incremental models with PostgreSQL-compatible SQL
- **Data Quality**: Built-in linting and validation

### Object Storage
- **S3-Compatible**: MinIO provides local object storage
- **Iceberg Tables**: Data files stored in Parquet/ORC format
- **Separation of Compute and Storage**: Query engine and storage are independent

## Prerequisites

- Docker and Docker Compose
- Python 3.12 or higher
- UV package manager (recommended) or pip

## Getting Started

### 1. Start Infrastructure

Launch all services using Docker Compose:

```bash
docker-compose up -d
```

This will start:
- PostgreSQL with pg_lake (port 5432) - Iceberg catalog
- pgduck_server (port 5332) - DuckDB query engine
- MinIO (ports 9000, 9001) - Object storage
- Automatic setup containers for initialization

### 2. Verify Services

Check that all services are healthy:

```bash
docker-compose ps
```

Access the web interfaces:
- **MinIO Console**: http://localhost:9001 (credentials in docker-compose.yml)
- **PostgreSQL (pg_lake)**: localhost:5432 (psql client or any PostgreSQL tool)
- **pgduck_server**: localhost:5332 (psql client or any PostgreSQL tool)

### 3. Install Python Dependencies

```bash
# Using uv (recommended)
uv sync

# Or using pip
pip install -e .
```

### 4. Run SQLMesh

Activate your virtual environment and run SQLMesh commands:

```bash
# Activate environment
source .venv/bin/activate

# Navigate to SQLMesh project
cd sqlmesh

# Plan and apply transformations
sqlmesh plan

# Run in development environment
sqlmesh plan dev_<your_name>

# Apply to production
sqlmesh plan prod
```

## Configuration Details

### PostgreSQL (pg_lake) - Catalog

- **Host**: localhost:5432
- **Database**: lakehouse
- **User**: lakehouse_user
- **Extensions**: pg_lake
- **Role**: Iceberg catalog and metadata management

### pgduck_server - Query Engine

- **Host**: localhost:5332
- **Protocol**: PostgreSQL wire protocol
- **Backend**: DuckDB
- **Role**: High-performance analytical query execution
- **Access**: Connect using any PostgreSQL client (psql, DBeaver, DataGrip, etc.)

### MinIO Storage

- **API Endpoint**: http://localhost:9000
- **Console**: http://localhost:9001
- **Buckets**: warehouse (Iceberg data)

## Development Workflow

1. **Create Models**: Add SQL files to `sqlmesh/models/`
2. **Define Transformations**: Use SQLMesh model syntax with incremental strategies
3. **Test Locally**: Run `sqlmesh plan` to preview changes
4. **Deploy**: Apply changes to target environment
5. **Query**: Use any PostgreSQL client to query Iceberg tables via pgduck_server

### Example Model

```sql
MODEL (
  name staging.hello,
  kind INCREMENTAL_BY_UNIQUE_KEY (
    unique_key id
  )
);

SELECT
    1 as id,
    'iqbal' as name,
    CURRENT_TIMESTAMP as created_at
```

### Querying Iceberg Tables

```sql
-- Connect to pgduck_server (DuckDB backend)
psql -h localhost -p 5332 -U duckdb

-- Query Iceberg tables with DuckDB's performance
SELECT * FROM staging.hello;

-- Complex analytical queries benefit from DuckDB's columnar engine
SELECT 
    date_trunc('day', created_at) as day,
    count(*) as count
FROM staging.hello
GROUP BY 1
ORDER BY 1;

-- DuckDB's advanced analytics capabilities
SELECT 
    name,
    count(*) OVER (PARTITION BY name) as name_count,
    row_number() OVER (ORDER BY created_at) as row_num
FROM staging.hello;
```

## Scaling to Cloud

While this POC runs entirely on your local machine, the architecture is designed to scale seamlessly to cloud infrastructure. Here's how the components map:

### Local → Cloud Migration Path

| Local Component | Cloud Equivalent | Notes |
|----------------|------------------|-------|
| **pg_lake (PostgreSQL)** | AWS Glue, Nessie, Polaris Catalog, Unity Catalog | Purpose-built catalog services or managed PostgreSQL |
| **pgduck_server (DuckDB)** | Trino, Presto, Spark, Athena, BigQuery | Distributed query engines for larger scale |
| **MinIO** | AWS S3, GCS, Azure Blob Storage | Drop-in replacement (S3-compatible API) |
| **SQLMesh** | SQLMesh (same) | Runs identically in cloud with different connection strings |

### Cloud Scaling Strategies

#### Option 1: Managed Services (Easiest)
- **Catalog**: AWS Glue Data Catalog, Databricks Unity Catalog, or Polaris
- **Query Engine**: Amazon Athena, Starburst, or managed Trino/Presto
- **Storage**: S3, GCS, or Azure Blob Storage
- **SQLMesh**: Run on CI/CD runners (GitHub Actions, GitLab CI)

#### Option 2: Kubernetes (Most Flexible)
- Deploy catalog service (Polaris, Nessie) on Kubernetes
- Deploy Trino/Presto/Spark clusters for distributed queries
- Use cloud object storage (S3/GCS/Azure)
- Scale query engines independently based on workload

#### Option 3: Hybrid (Cost-Effective)
- Keep lightweight catalog service (managed PostgreSQL or Nessie)
- Add distributed query engine only when data volume requires it
- Use cloud object storage from day one
- Start with serverless queries (Athena) before moving to always-on clusters

### Key Advantages of This Approach

1. **Same Iceberg Format**: Tables created locally work in cloud without modification
2. **Standard SQL**: SQLMesh models run on any SQL engine (PostgreSQL, DuckDB, Trino, Spark)
3. **S3 API Compatibility**: MinIO to S3 migration is just a config change
4. **Learn Once, Scale Later**: Master concepts locally, scale when needed
5. **PostgreSQL Protocol**: pgduck_server compatibility means any PostgreSQL tool works

### When to Scale

Consider moving to cloud infrastructure when:
- Data volume exceeds 100s of GBs (single-node DuckDB becomes slow)
- Need multi-user concurrent query workloads (100+ concurrent users)
- Require high availability and disaster recovery
- Team grows beyond 5-10 data practitioners
- Need integration with cloud-native services (Lambda, EMR, Glue, etc.)
- Query complexity requires distributed processing (joins on TB+ datasets)

## Data Quality

SQLMesh provides:
- SQL linting and validation
- Model testing frameworks
- Audit definitions for data quality checks
- Incremental model strategies to handle large datasets efficiently

## Troubleshooting

### Services Not Starting

Check logs for specific services:
```bash
docker-compose logs <service_name>
```

### PostgreSQL (pg_lake) Connection Issues

Test catalog connection:
```bash
docker-compose exec pg_lake psql -U lakehouse_user -d lakehouse
```

### pgduck_server Connection Issues

Test query engine connection:
```bash
psql -h localhost -p 5332 -U duckdb
```

### Storage Access Problems

Check MinIO bucket permissions in the MinIO console (http://localhost:9001).

### Query Performance Issues

DuckDB in pgduck_server excels at:
- Columnar data (Parquet, ORC)
- Aggregations and window functions
- Complex joins on moderate datasets

For very large datasets (10+ TB), consider scaling to distributed engines like Trino or Spark.

## Dependencies

Key Python packages:
- `sqlmesh[postgres]` - Data transformation framework with PostgreSQL support
- `psycopg2-binary` - PostgreSQL adapter for both pg_lake and pgduck_server
- `ipykernel` - Jupyter notebook support (optional)

## Future Enhancements

- [ ] Add data ingestion pipelines (CSV, JSON, APIs)
- [ ] Implement comprehensive data quality audits
- [ ] Add CI/CD pipeline integration (GitHub Actions)
- [ ] Create example dashboards (Metabase, Superset)
- [ ] Add data lineage visualization
- [ ] Implement incremental snapshot strategies
- [ ] Add monitoring and alerting
- [ ] Document production deployment patterns
- [ ] Add MotherDuck integration option for cloud DuckDB

## Why pg_lake + pgduck_server?

**Simplicity**: Two focused services instead of coordinating multiple complex systems (Polaris + Trino + separate state DB).

**Performance**: pgduck_server leverages DuckDB's state-of-the-art columnar execution engine, often outperforming traditional databases on analytical queries.

**Cost**: Fewer services to run and manage, significantly lower infrastructure costs for small-to-medium workloads.

**Developer Experience**: Standard PostgreSQL protocol means any PostgreSQL tool works with pgduck_server - no special drivers or clients needed.

**Production Ready**: PostgreSQL is battle-tested, DuckDB is actively maintained and used in production by many organizations.

**Future Proof**: Based on open standards (Iceberg, PostgreSQL protocol), easy to swap components later.

**Local Development**: Run the entire stack on a laptop, no cloud accounts or complex setup required.

## Key Concepts

### What is pgduck_server?

pgduck_server is a specialized server that:
- Listens on port 5332 (default) using the PostgreSQL wire protocol
- Transparently executes all queries on DuckDB instead of PostgreSQL
- Allows any PostgreSQL client to leverage DuckDB's performance
- Provides compatibility with existing PostgreSQL tools and libraries

This means you can connect with `psql`, DataGrip, DBeaver, Python's `psycopg2`, or any other PostgreSQL client, and you'll automatically get DuckDB's performance for analytical queries.

### Separation of Catalog and Query Engine

**Catalog (pg_lake)**: Stores metadata about tables, schemas, partitions, and data locations. Lightweight and doesn't process data.

**Query Engine (pgduck_server/DuckDB)**: Reads metadata from catalog, fetches data from object storage, and executes queries. This is where the computational work happens.

This separation allows you to:
- Scale catalog and query engine independently
- Use different engines for different workloads
- Upgrade one component without affecting the other

## License

This is a proof-of-concept project for evaluation purposes.

## References

- [pg_lake Documentation](https://github.com/Snowflake-Labs/pg_lake)
- [pgduck_server Documentation](https://github.com/Snowflake-Labs/pg_lake/tree/main/pgduck_server)
- [SQLMesh Documentation](https://sqlmesh.readthedocs.io/)
- [Apache Iceberg](https://iceberg.apache.org/)
- [MinIO Documentation](https://min.io/docs/)
- [DuckDB](https://duckdb.org/)
